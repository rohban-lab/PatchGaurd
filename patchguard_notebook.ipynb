{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from scipy.ndimage import gaussian_filter, map_coordinates\n",
    "import cv2\n",
    "import noise\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFilter\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bounds = {\n",
    "    \"mvtec\" : {\n",
    "        \"toothbrush\":(14, 39), \"cable\":(14, 65), \"screw\":(9, 35), \"transistor\":(14, 86), \"capsule\":(14, 39) , \"bottle\":(14, 65), \"hazelnut\":(14, 65), \"metal_nut\":(14, 65), \"pill\":(14, 39), \"zipper\":(14, 65),    \n",
    "        \"wood\":(14, 86), \"carpet\":(14, 86), \"grid\":(14, 86), \"leather\":(14, 86), \"tile\":(14, 86)\n",
    "    },\n",
    "    'visa' : {\n",
    "        'candle':(14, 86), 'capsules':(6, 86), 'cashew':(10, 86), 'chewinggum':(14, 86), 'fryum':(6, 15), 'macaroni1':(14, 86),\n",
    "        'macaroni2':(14, 86), 'pcb1':(14, 86), 'pcb2':(14, 86), 'pcb3':(14, 86), 'pcb4':(14, 86), 'pipe_fryum':(14, 86)\n",
    "    },\n",
    "    'mpdd' : {\n",
    "        'bracket_black':(6, 86), 'bracket_brown':(6, 86), 'bracket_white':(6, 86), 'connector':(6, 86), 'metal_plate':(14, 86), 'tubes':(6, 86) \n",
    "    },\n",
    "    'btad' : {\n",
    "        '01':(9, 86), '02':(14, 86), '03':(9, 86)\n",
    "    },\n",
    "    'dtd' : {\n",
    "        'Blotchy_099':(14, 86) , 'Fibrous_183':(14, 86) , 'Marbled_078':(14, 86) , 'Matted_069':(14, 86) , 'Mesh_114':(14, 86) , 'Perforated_037':(14, 86) , 'Stratified_154':(14, 86) , 'Woven_001':(14, 86) , 'Woven_068':(14, 86) , 'Woven_104':(14, 86) , 'Woven_125':(14, 86) , 'Woven_127':(14, 86)\n",
    "    },\n",
    "    'brats2021' : {\n",
    "        \"\":(14, 86)\n",
    "    },\n",
    "    'headct' : {\n",
    "        \"\":(9, 86)\n",
    "    },\n",
    "    'wfdd' : {\n",
    "        \"grey_cloth\":(9, 86), \"grid_cloth\":(9, 86), \"pink_flower\":(9, 86), \"yellow_cloth\":(9, 86)\n",
    "    }\n",
    "}\n",
    "\n",
    "class RandomAugmentations:\n",
    "    def __init__(self, seed=None):\n",
    "        self.seed = seed\n",
    "\n",
    "        self.param_ranges = {\n",
    "            'brightness': {'light': (0.1, 0.1), 'medium': (0.4, 0.4), 'heavy': (0.8, 1)},\n",
    "            'contrast':   {'light': (0.1, 0.1), 'medium': (0.4, 0.4),  'heavy': (0.8, 1)},\n",
    "            'saturation': {'light': (0.1, 0.1), 'medium': (0.4, 0.4), 'heavy': (0.8, 1)},\n",
    "            'hue':        {'light': (0.1, 0.1), 'medium': (0.3, 0.3), 'heavy': (0.5, 0.5)},\n",
    "\n",
    "            'elastic_alpha': {'light': (10, 20), 'medium': (20, 40), 'heavy': (40, 100)},\n",
    "\n",
    "            'torn_lines': {'light': (1, 3), 'medium': (5, 10), 'heavy': (10, 20)},\n",
    "            \n",
    "            'perlin_scale': {'light': (20, 50), 'medium': (10, 20), 'heavy': (5, 10)},\n",
    "            'perlin_threshold': {'light': (200, 255), 'medium': (150, 200), 'heavy': (128, 150)},\n",
    "\n",
    "            'swirl_strength': {'light': (0.5, 1.0), 'medium': (1.0, 1.5), 'heavy': (1.5, 2)},\n",
    "\n",
    "            'erase_ratio': {'light': (0.01, 0.05), 'medium': (0.05, 0.1), 'heavy': (0.1, 0.2)},\n",
    "            'erase_rects': {'light': (1, 2), 'medium': (2, 3), 'heavy': (3, 5)},\n",
    "\n",
    "            'blur_radius': {'light': (0.2, 0.5), 'medium': (0.8, 1.5), 'heavy': (2.0, 3)},\n",
    "\n",
    "            'jpeg_quality': {'light': (30, 50), 'medium': (20, 30), 'heavy': (1, 20)},\n",
    "        }\n",
    "\n",
    "        self.num_augmentations = {'light': 1, 'medium': 2, 'heavy': 4}\n",
    "\n",
    "        self.augmentations = {\n",
    "            \"light\": [\n",
    "                self.gaussian_blur, self.elastic_transform, self.swirl_distortion,\n",
    "            ],\n",
    "            \"medium\": [\n",
    "                self.gaussian_blur, self.swirl_distortion, self.jpeg_artifacts, \n",
    "                self.elastic_transform\n",
    "            ],\n",
    "            \"heavy\": [\n",
    "                self.elastic_transform, self.torn_paper_effect, self.jpeg_artifacts,\n",
    "                self.perlin_noise_mask, self.swirl_distortion, self.random_erasing, \n",
    "                self.gaussian_blur,\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "    def apply(self, image, level='medium'):\n",
    "        image_np = np.array(image)\n",
    "        n_augmentations = random.randint(0, self.num_augmentations[level])\n",
    "        selected_augmentations = random.sample(self.augmentations[level], n_augmentations)\n",
    "        \n",
    "        selected_augmentations.insert(random.randint(0, len(selected_augmentations)), self.color_transformation)\n",
    "\n",
    "        for augmentation in selected_augmentations:\n",
    "            image_np = augmentation(image_np, level)\n",
    "\n",
    "        return Image.fromarray(image_np)\n",
    "\n",
    "    def elastic_transform(self, image, level):\n",
    "        alpha = random.uniform(*self.param_ranges['elastic_alpha'][level])\n",
    "        sigma = 3.0\n",
    "        random_state = np.random.RandomState(self.seed)\n",
    "        shape = image.shape\n",
    "\n",
    "        dx = gaussian_filter((random_state.rand(*shape[:2]) * 2 - 1), sigma, mode=\"reflect\") * alpha\n",
    "        dy = gaussian_filter((random_state.rand(*shape[:2]) * 2 - 1), sigma, mode=\"reflect\") * alpha\n",
    "        x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
    "        indices = (y + dy).flatten(), (x + dx).flatten()\n",
    "\n",
    "        distorted_image = np.zeros_like(image)\n",
    "        for i in range(shape[2]):\n",
    "            distorted_image[..., i] = map_coordinates(image[..., i], indices, order=1, mode='reflect').reshape(shape[:2])\n",
    "        return distorted_image\n",
    "\n",
    "    def torn_paper_effect(self, image, level):\n",
    "        image_np = image.copy()\n",
    "        height, width = image_np.shape[:2]\n",
    "        num_lines = random.randint(*self.param_ranges['torn_lines'][level])\n",
    "        for _ in range(num_lines):\n",
    "            start_x = np.random.randint(0, width)\n",
    "            start_y = np.random.randint(0, height)\n",
    "            end_x = np.random.randint(0, width)\n",
    "            end_y = np.random.randint(0, height)\n",
    "            cv2.line(image_np, (start_x, start_y), (end_x, end_y), [random.choice([0, 255]) for _ in range(3)], thickness=1)\n",
    "        return image_np\n",
    "\n",
    "    def perlin_noise_mask(self, image, level):\n",
    "        scale = random.uniform(*self.param_ranges['perlin_scale'][level])\n",
    "        threshold = random.randint(*self.param_ranges['perlin_threshold'][level])\n",
    "        height, width = image.shape[:2]\n",
    "        mask = np.zeros((height, width), dtype=np.float32)\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                mask[i, j] = noise.pnoise2(i / scale, j / scale, octaves=6)\n",
    "        mask = (mask - mask.min()) / (mask.max() - mask.min()) * 255\n",
    "        image[mask > threshold] = np.random.randint(0, 255, 3)\n",
    "        return image\n",
    "\n",
    "    def color_transformation(self, image, level):\n",
    "        b = random.uniform(*self.param_ranges['brightness'][level])\n",
    "        c = random.uniform(*self.param_ranges['contrast'][level])\n",
    "        s = random.uniform(*self.param_ranges['saturation'][level])\n",
    "        h = random.uniform(*self.param_ranges['hue'][level])\n",
    "        transform = transforms.ColorJitter(brightness=b, contrast=c, saturation=s, hue=h)\n",
    "        return np.array(transform(Image.fromarray(image)))\n",
    "\n",
    "    def swirl_distortion(self, image, level):\n",
    "        strength = random.uniform(*self.param_ranges['swirl_strength'][level])\n",
    "        patch_np = np.array(image)\n",
    "        height, width = patch_np.shape[:2]\n",
    "        center_x, center_y = width // 2, height // 2\n",
    "        y, x = np.indices((height, width))\n",
    "        x = x - center_x\n",
    "        y = y - center_y\n",
    "        distance = np.sqrt(x**2 + y**2)\n",
    "        angle = strength * np.exp(-distance**2 / (2 * (min(height, width) // 3)**2))\n",
    "        new_x = center_x + x * np.cos(angle) - y * np.sin(angle)\n",
    "        new_y = center_y + x * np.sin(angle) + y * np.cos(angle)\n",
    "        map_x = np.clip(new_x, 0, width - 1).astype(np.float32)\n",
    "        map_y = np.clip(new_y, 0, height - 1).astype(np.float32)\n",
    "        return cv2.remap(patch_np, map_x, map_y, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    def random_erasing(self, image, level):\n",
    "        image_np = image.copy()\n",
    "        h, w = image_np.shape[:2]\n",
    "        erase_area_ratio = random.uniform(*self.param_ranges['erase_ratio'][level])\n",
    "        num_rectangles = random.randint(*self.param_ranges['erase_rects'][level])\n",
    "        for _ in range(num_rectangles):\n",
    "            erase_area = int(erase_area_ratio * h * w)\n",
    "            erase_aspect_ratio = random.uniform(0.3, 3.3)\n",
    "            erase_height = int(np.sqrt(erase_area / erase_aspect_ratio))\n",
    "            erase_width = int(erase_aspect_ratio * erase_height)\n",
    "            x = random.randint(0, max(0, w - erase_width))\n",
    "            y = random.randint(0, max(0, h - erase_height))\n",
    "            image_np[y:y+erase_height, x:x+erase_width] = np.random.randint(0, 255, 3)\n",
    "        return image_np\n",
    "\n",
    "    def gaussian_blur(self, image, level):\n",
    "        radius = random.uniform(*self.param_ranges['blur_radius'][level])\n",
    "        return np.array(Image.fromarray(image).filter(ImageFilter.GaussianBlur(radius=radius)))\n",
    "\n",
    "    def jpeg_artifacts(self, image, level):\n",
    "        quality = random.randint(*self.param_ranges['jpeg_quality'][level])\n",
    "        pil_image = Image.fromarray(image)\n",
    "        buffer = io.BytesIO()\n",
    "        pil_image.save(buffer, format='JPEG', quality=quality)\n",
    "        return np.array(Image.open(buffer))\n",
    "    \n",
    "\n",
    "class AnomalyGenerator(object):\n",
    "    def __init__(self, dataset, class_name, seed):\n",
    "        self.lower_bound, self.upper_bound = bounds[dataset][class_name]\n",
    "\n",
    "        self.random_augmentor = RandomAugmentations(seed=seed)\n",
    "\n",
    "    def rotate(self, patch, width, height, min_angle=-90, max_angle=90):\n",
    "        random_rotate = random.uniform(min_angle, max_angle)\n",
    "        patch = patch.convert(\"RGBA\").rotate(random_rotate, expand=True)\n",
    "        patch = patch.resize((width, height), resample=Image.BICUBIC)\n",
    "        mask = patch.split()[-1]\n",
    "        \n",
    "        return patch.convert(\"RGB\"), mask\n",
    "\n",
    "    def intersect_masks(self, mask1, mask2):\n",
    "        mask1_np = np.array(mask1)\n",
    "        mask2_np = np.array(mask2)\n",
    "    \n",
    "        intersection = np.logical_and(mask1_np, mask2_np).astype(np.uint8) * 255\n",
    "        intersection_mask = Image.fromarray(intersection)\n",
    "    \n",
    "        return intersection_mask\n",
    "    \n",
    "    def get_max_shape(self, x, y, foreground_mask):\n",
    "        max_width = 0\n",
    "        for i in range(x, foreground_mask.shape[1]):\n",
    "            if foreground_mask[y, i] == 1:\n",
    "                max_width += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        max_height = 0\n",
    "        for j in range(y, foreground_mask.shape[0]):\n",
    "            if foreground_mask[j, x] == 1:\n",
    "                max_height += 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        return max_width, max_height\n",
    "    \n",
    "    def sample_patch_size(self, foreground_mask, x1, y1, x2, y2, max_width, max_height):\n",
    "        num_attempts = 0\n",
    "        while num_attempts <= 10:\n",
    "            patch_width = random.randint(0, max_width)\n",
    "            patch_height = random.randint(0, max_height)\n",
    "\n",
    "            patch_region_src = foreground_mask[y1:y1+patch_height, x1:x1+patch_width]\n",
    "            patch_region_dst = foreground_mask[y2:y2+patch_height, x2:x2+patch_width]\n",
    "\n",
    "            if np.all(patch_region_src == 1) and np.all(patch_region_dst == 1):\n",
    "                break\n",
    "                \n",
    "            num_attempts += 1\n",
    "\n",
    "        return patch_width, patch_height\n",
    "    \n",
    "    def expand_mask(self, mask, kernel_size=(3, 3)):\n",
    "        kernel = np.ones(kernel_size, np.uint8)\n",
    "        expanded_mask = cv2.dilate(mask.astype(np.uint8), kernel, iterations=10)\n",
    "        \n",
    "        return expanded_mask\n",
    "    \n",
    "    def sample_coordinate_shape(self, foreground_mask):\n",
    "        foreground_mask = self.expand_mask(foreground_mask)\n",
    "        \n",
    "        h, w = foreground_mask.shape\n",
    "        coords = np.column_stack(np.where(foreground_mask == 1))\n",
    "        \n",
    "        num_attempts = 0\n",
    "        while num_attempts <= 250:\n",
    "            y1, x1 = coords[random.randint(0, len(coords) - 1)]\n",
    "            max_width1, max_height1 = self.get_max_shape(x1, y1, foreground_mask)       \n",
    "\n",
    "            y2, x2 = coords[random.randint(0, len(coords) - 1)]\n",
    "            max_width2, max_height2 = self.get_max_shape(x2, y2, foreground_mask) \n",
    "\n",
    "            max_width, max_height = min(max_width1, max_width2), min(max_height1, max_height2)\n",
    "\n",
    "            patch_width, patch_height = self.sample_patch_size(foreground_mask, x1, y1, x2, y2, max_width, max_height)\n",
    "\n",
    "            if patch_width < self.lower_bound or patch_height < self.lower_bound:\n",
    "                num_attempts += 1\n",
    "\n",
    "            else:\n",
    "                patch_region_src = foreground_mask[y1:y1+patch_height, x1:x1+patch_width]\n",
    "                patch_region_dst = foreground_mask[y2:y2+patch_height, x2:x2+patch_width]\n",
    "\n",
    "                if np.all(patch_region_src == 1) and np.all(patch_region_dst == 1) and (x2 + patch_width <= w and y2 + patch_height <= h):\n",
    "                    break\n",
    "\n",
    "                num_attempts += 1\n",
    "        \n",
    "        if num_attempts > 250:\n",
    "            y1, x1 = coords[random.randint(0, len(coords) - 1)]\n",
    "            y2, x2 = coords[random.randint(0, len(coords) - 1)]\n",
    "            patch_width, patch_height = self.lower_bound, self.lower_bound\n",
    "\n",
    "        return x1, y1, x2, y2, patch_width, patch_height\n",
    "            \n",
    "    def __call__(self, imgs, foreground_masks):\n",
    "        batch_size, _, h, w = imgs.shape\n",
    "        transformed_imgs = []\n",
    "        transformed_masks = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            img = imgs[i].cpu()\n",
    "            img_pil = transforms.ToPILImage()(img)\n",
    "            foreground_mask = foreground_masks[i].cpu().squeeze(0).numpy()\n",
    "\n",
    "            x1, y1, x2, y2, patch_width, patch_height = self.sample_coordinate_shape(foreground_mask)\n",
    "\n",
    "            patch = img_pil.crop((x1, y1, x1 + int(patch_width), y1 + int(patch_height)))\n",
    "\n",
    "            # transformations \n",
    "            patch = self.random_augmentor.apply(patch, np.random.choice(['light', 'medium', 'heavy'], p=[0.2,0.2,0.6]))\n",
    "\n",
    "            patch, rotation_mask = self.rotate(patch, patch_width, patch_height)\n",
    "            \n",
    "            mask = np.ones((int(patch_height), int(patch_width)), dtype=np.uint8)\n",
    "            mask = cv2.resize(mask, (int(patch_width), int(patch_height)), interpolation=cv2.INTER_CUBIC)\n",
    "            mask = self.intersect_masks(mask, rotation_mask)\n",
    "                                    \n",
    "            augmented = img_pil.copy()\n",
    "\n",
    "            augmented.paste(patch, (x2, y2), mask=mask)\n",
    "\n",
    "            org_mask = Image.fromarray(np.zeros((h, w), dtype='uint8')).convert('L')\n",
    "            org_mask.paste(mask, (x2, y2))\n",
    "            \n",
    "            augmented = transforms.ToTensor()(augmented)\n",
    "            org_mask = transforms.ToTensor()(org_mask)\n",
    "\n",
    "            transformed_imgs.append(augmented)\n",
    "            transformed_masks.append(org_mask)\n",
    "\n",
    "        transformed_imgs = torch.stack(transformed_imgs)\n",
    "        transformed_masks = torch.stack(transformed_masks)\n",
    "\n",
    "        return transformed_imgs, transformed_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchGuard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def my_forward_wrapper(attn_obj):\n",
    "    def my_forward(x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = attn_obj.qkv(x).reshape(B, N, 3, attn_obj.num_heads, C // attn_obj.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   \n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * attn_obj.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = attn_obj.attn_drop(attn)\n",
    "        attn_obj.attn_map = attn\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = attn_obj.proj(x)\n",
    "        x = attn_obj.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    return my_forward\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, hf_path, feature_layer_indices, reg_layer_indices, image_size, device):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        self.mu = torch.tensor(mean).view(1, 3, 1, 1).to(device)\n",
    "        self.std = torch.tensor(std).view(1, 3, 1, 1).to(device)\n",
    "        self.norm = lambda x: (x - self.mu) / self.std\n",
    "\n",
    "        self.feature_layer_indices = feature_layer_indices\n",
    "        self.reg_layer_indices = reg_layer_indices\n",
    "\n",
    "        self.pretrained_model = timm.create_model(hf_path, pretrained=False, num_classes=0, img_size=image_size).to(device)\n",
    "\n",
    "        self.embed_dim = len(feature_layer_indices) * self.pretrained_model.embed_dim\n",
    "        self.patch_size = self.pretrained_model.patch_embed.patch_size[0]\n",
    "        self.num_patches = (image_size // self.patch_size) ** 2\n",
    "\n",
    "        pattern = r'reg(\\d+)'\n",
    "        match = re.search(pattern, hf_path)\n",
    "        self.start_index = int(match.group(1)) + 1 if match else 1\n",
    "\n",
    "        indices = set(feature_layer_indices + reg_layer_indices)\n",
    "        for i in indices:\n",
    "            self.pretrained_model.blocks[i-1].attn.forward = my_forward_wrapper(self.pretrained_model.blocks[i-1].attn)\n",
    "            \n",
    "    def forward(self, x, use_reg=True):\n",
    "        x = self.norm(x)\n",
    "        x = self.pretrained_model.patch_embed(x)\n",
    "        x = self.pretrained_model._pos_embed(x)\n",
    "        x = self.pretrained_model.patch_drop(x)\n",
    "        x = self.pretrained_model.norm_pre(x)\n",
    "\n",
    "        out = []\n",
    "        attention_weights = []\n",
    "\n",
    "        # iterating through the layers up to last layer to extract from => 12 layers\n",
    "        for idx, layer in enumerate(self.pretrained_model.blocks, start=1):\n",
    "            x = layer(x)\n",
    "\n",
    "            if idx in self.feature_layer_indices:\n",
    "                features_layer = self.pretrained_model.norm(x[:, self.start_index:, :])\n",
    "                out.append(features_layer)\n",
    "\n",
    "            if use_reg and idx in self.reg_layer_indices:\n",
    "                attention_map = layer.attn.attn_map\n",
    "                attention_weights.append(attention_map[:, :, 1:, 1:])  # Remove CLS token\n",
    "\n",
    "            if idx == max(self.feature_layer_indices):  \n",
    "                break\n",
    "\n",
    "        features = torch.cat(out, dim=-1)\n",
    "\n",
    "        return (features, attention_weights) if use_reg else (features, None)\n",
    "                \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = self.layer_norm_1(x + self.dropout1(attn_output))\n",
    "        x = x + self.dropout2(self.linear(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_patches, num_layers=1, num_heads=12, dropout_rate=0):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout_rate) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 1)\n",
    "        )\n",
    "        self.positional_encodings = nn.Parameter(torch.randn(num_patches, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.positional_encodings.unsqueeze(0)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output_layer(x).squeeze(-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class PatchGuard(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = FeatureExtractor(args[\"hf_path\"], args[\"feature_layers\"], args[\"reg_layers\"], args[\"image_size\"], device)\n",
    "\n",
    "        embed_dim = self.feature_extractor.embed_dim\n",
    "        self.num_patches = self.feature_extractor.num_patches\n",
    "        self.patch_size = self.feature_extractor.patch_size\n",
    "        self.patches_per_side = int(np.sqrt(self.num_patches))\n",
    "\n",
    "        self.discriminator = Discriminator(embed_dim, args[\"hidden_dim\"], self.num_patches, args[\"dsc_layers\"], args[\"dsc_heads\"], 0.2).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings, _ = self.feature_extractor(x, False)\n",
    "        scores = self.discriminator(embeddings)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pgd_attack(model, images, masks, epsilon, num_iter):\n",
    "    alpha =  (2.5 * epsilon) / num_iter\n",
    "\n",
    "    X = images.clone().detach()\n",
    "    original_X = X.data\n",
    "\n",
    "    for i in range(num_iter) :    \n",
    "        X.requires_grad = True\n",
    "\n",
    "        scores = model(X)\n",
    "\n",
    "        zeros_count = (masks == 0).sum(dim=1)\n",
    "        non_zeros_count = (masks != 0).sum(dim=1)\n",
    "\n",
    "        anomalous_loss = (masks * scores).sum(dim=1) / (non_zeros_count + 1e-8)\n",
    "        normal_loss = ((1 - masks) * scores).sum(dim=1) / (zeros_count + 1e-8)\n",
    "        loss = normal_loss.sum() - anomalous_loss.sum()\n",
    "        loss.backward()\n",
    "\n",
    "        adv_X = X + alpha * X.grad.sign()\n",
    "        delta = torch.clamp(adv_X - original_X, min=-epsilon, max=epsilon)\n",
    "        X = torch.clamp(original_X + delta, min=0, max=1).detach_()\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, reg_type, device, num_patches):\n",
    "        super(Loss, self).__init__()\n",
    "        self.reg_type = reg_type\n",
    "        self.device = device\n",
    "        self.num_patches = num_patches\n",
    "        self.reg_weights = {\"KL_divergence\":0.01, \"L2_norm\":0.1, \"soft_R\":0.1, \"R\":0.1}\n",
    "        self.reg_hypers = {\"R\":{\"tau\":0.01}}\n",
    "\n",
    "    def forward(self, scores, masks, attn_weights=None):\n",
    "        loss_per_patch = F.binary_cross_entropy_with_logits(scores, masks, reduction='none')\n",
    "        loss_per_image = loss_per_patch.sum(dim=1)\n",
    "\n",
    "        # zeros_count = (masks == 0).sum(dim=1)\n",
    "        # non_zeros_count = (masks != 0).sum(dim=1)\n",
    "\n",
    "        # anomalous_loss = (masks * loss_per_patch).sum(dim=1) / (non_zeros_count + 1e-8)\n",
    "        # normal_loss = ((1 - masks) * loss_per_patch).sum(dim=1) / (zeros_count + 1e-8)\n",
    "\n",
    "        # loss_per_image = normal_loss.sum() + anomalous_loss.sum()\n",
    "\n",
    "        reg_term = 0\n",
    "        if attn_weights is not None:\n",
    "            reg_term = self.reg(attn_weights)  \n",
    "\n",
    "        total_loss = loss_per_image.mean() + 1 * reg_term\n",
    "        return total_loss\n",
    "\n",
    "    def reg(self, attn_weights):\n",
    "        reg_term = 0\n",
    "        coefs = [0.005, 0.01, 0.05]\n",
    "        if self.reg_type == \"KL_divergence\":\n",
    "            for i, attn_weight in enumerate(attn_weights):\n",
    "                reg_term += coefs[i] * F.kl_div(torch.log(attn_weight + 1e-8),  torch.full_like(attn_weight, 1 / self.num_patches).to(self.device), reduction='mean')\n",
    "        \n",
    "        # not recommended        \n",
    "        elif self.reg_type == \"L2_norm\":\n",
    "            for attn_weight in attn_weights:\n",
    "                reg_term += attn_weight.norm(p='fro')\n",
    "        elif self.reg_type == \"soft_R\":\n",
    "            for attn_weight in attn_weights:\n",
    "                soft_mask = torch.sigmoid((self.delta - attn_weight) / self.reg_hypers[\"R\"][\"tau\"])  # Smooth threshold\n",
    "                reg_term += 1 / (torch.sum(attn_weight * soft_mask) + 1e-8)\n",
    "                #reg_term -= torch.sum(attn_weight * soft_mask)\n",
    "        elif self.reg_type == \"R\":\n",
    "            for attn_weight in attn_weights:\n",
    "                mask = (attn_weight <= 1 / self.num_patches).float() # Hard threshold\n",
    "                reg_term += 1 / (torch.sum(attn_weight * mask) + 1e-8) # Summing selected attention values\n",
    "                #reg_term -= torch.sum(attn_weight * mask) # Summing selected attention values\n",
    "\n",
    "        \n",
    "        return reg_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MVTec(data.Dataset):\n",
    "    def __init__(self, path, class_name, transform=None, mask_transform=None, seed=0, split='train', size=224):\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.data = []\n",
    "        self.size = size\n",
    "        self.has_fg_mask = class_name in ['bottle', 'cable', 'capsule', 'hazelnut', 'metal_nut', 'pill', 'screw', 'toothbrush', 'zipper']\n",
    "\n",
    "        path = os.path.join(path, class_name)\n",
    "        mv_str = '_mask.'\n",
    "\n",
    "        normal_dir = os.path.join(path, split, \"good\")\n",
    "        \n",
    "        if split == 'train' and self.has_fg_mask:\n",
    "            self.foreground_mask_path = os.path.join(path, split, \"foreground_mask\")\n",
    "            \n",
    "        for img_file in os.listdir(normal_dir):\n",
    "            image_dir = os.path.join(normal_dir, img_file)\n",
    "            foreground_mask_dir = None\n",
    "            if split == 'train' and self.has_fg_mask:\n",
    "                foreground_mask_dir = os.path.join(self.foreground_mask_path, img_file)\n",
    "            self.data.append((image_dir, None, foreground_mask_dir))\n",
    "            \n",
    "        if split == 'test':\n",
    "            test_dir = os.path.join(path, \"test\")\n",
    "            test_anomaly_dirs = []\n",
    "            for entry in os.listdir(test_dir):\n",
    "                full_path = os.path.join(test_dir, entry)\n",
    "\n",
    "                if os.path.isdir(full_path) and full_path != normal_dir:\n",
    "                    test_anomaly_dirs.append(full_path)\n",
    "\n",
    "            for dir in test_anomaly_dirs:\n",
    "                for img_file in os.listdir(dir):\n",
    "                    image_dir = os.path.join(dir, img_file)\n",
    "                    mask_dir = image_dir.replace(\"test\", \"ground_truth\")\n",
    "                    parts = mask_dir.rsplit('.', 1)\n",
    "                    mask_dir = parts[0] + mv_str + parts[1]\n",
    "                    self.data.append((image_dir, mask_dir, None))\n",
    "\n",
    "            random.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path, fore_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  \n",
    "\n",
    "        image = self.transform(image)          \n",
    "\n",
    "        if mask_path:\n",
    "            mask = Image.open(mask_path).convert('RGB')\n",
    "            mask = self.mask_transform(mask)\n",
    "            mask = 1.0 - torch.all(mask == 0, dim=0).float()\n",
    "            label = 1\n",
    "        else:\n",
    "            C, W, H = image.shape\n",
    "            mask = torch.zeros((H, W))\n",
    "            label = 0\n",
    "            \n",
    "        C, W, H = image.shape\n",
    "        foreground_mask = torch.ones((H, W))\n",
    "        if fore_path:\n",
    "            foreground_mask = Image.open(fore_path).convert('L')\n",
    "            foreground_mask = foreground_mask.resize((self.size, self.size), Image.NEAREST)\n",
    "            foreground_mask = transforms.ToTensor()(foreground_mask)\n",
    "            \n",
    "        return image, label, mask, foreground_mask\n",
    "    \n",
    "\n",
    "class MPDD(data.Dataset):\n",
    "    def __init__(self, path, class_name, transform=None, mask_transform=None, seed=0, split='train', size=224):\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.data = []\n",
    "        self.size = size\n",
    "        self.has_fg_mask = class_name in ['bracket_black', 'bracket_brown', 'bracket_white', 'connector', 'metal_plate', 'tubes']\n",
    "\n",
    "        path = os.path.join(path, class_name)\n",
    "        mv_str = '_mask.'\n",
    "\n",
    "        normal_dir = os.path.join(path, split, \"good\")\n",
    "        \n",
    "        if split == 'train' and self.has_fg_mask:\n",
    "            self.foreground_mask_path = os.path.join(path, split, \"foreground_mask\")\n",
    "            os.makedirs(self.foreground_mask_path, exist_ok=True)\n",
    "            \n",
    "        for img_file in os.listdir(normal_dir):\n",
    "            image_dir = os.path.join(normal_dir, img_file)\n",
    "            foreground_mask_dir = None\n",
    "            if split == 'train' and self.has_fg_mask:\n",
    "                foreground_mask_dir = os.path.join(self.foreground_mask_path, img_file)\n",
    "            self.data.append((image_dir, None, foreground_mask_dir))\n",
    "            \n",
    "        if split == 'test':\n",
    "            test_dir = os.path.join(path, \"test\")\n",
    "            test_anomaly_dirs = []\n",
    "            for entry in os.listdir(test_dir):\n",
    "                full_path = os.path.join(test_dir, entry)\n",
    "\n",
    "                if os.path.isdir(full_path) and full_path != normal_dir:\n",
    "                    test_anomaly_dirs.append(full_path)\n",
    "\n",
    "            for dir in test_anomaly_dirs:\n",
    "                for img_file in os.listdir(dir):\n",
    "                    image_dir = os.path.join(dir, img_file)\n",
    "                    mask_dir = image_dir.replace(\"test\", \"ground_truth\")\n",
    "                    parts = mask_dir.rsplit('.', 1)\n",
    "                    mask_dir = parts[0] + mv_str + parts[1]\n",
    "                    self.data.append((image_dir, mask_dir, None))\n",
    "\n",
    "            random.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path, fore_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  \n",
    "\n",
    "        image = self.transform(image)          \n",
    "\n",
    "        if mask_path:\n",
    "            mask = Image.open(mask_path).convert('RGB')\n",
    "            mask = self.mask_transform(mask)\n",
    "            mask = 1.0 - torch.all(mask == 0, dim=0).float()\n",
    "            label = 1\n",
    "        else:\n",
    "            C, W, H = image.shape\n",
    "            mask = torch.zeros((H, W))\n",
    "            label = 0\n",
    "            \n",
    "        C, W, H = image.shape\n",
    "        foreground_mask = torch.ones((H, W))\n",
    "        if fore_path:\n",
    "            foreground_mask = Image.open(fore_path).convert('L')\n",
    "            foreground_mask = foreground_mask.resize((self.size, self.size), Image.NEAREST)\n",
    "            foreground_mask = transforms.ToTensor()(foreground_mask)\n",
    "            \n",
    "        return image, label, mask, foreground_mask\n",
    "    \n",
    "class BTAD(data.Dataset):\n",
    "    def __init__(self, path, class_name, transform=None, mask_transform=None, seed=0, split='train', size=224):\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.data = []\n",
    "        self.size = size\n",
    "        self.has_fg_mask = class_name in ['01', '03']\n",
    "\n",
    "        path = os.path.join(path, class_name)\n",
    "        \n",
    "        normal_dir = os.path.join(path, split, \"ok\")\n",
    "        \n",
    "        if split == 'train' and self.has_fg_mask:\n",
    "            self.foreground_mask_path = os.path.join(path, split, \"foreground_mask\")\n",
    "            os.makedirs(self.foreground_mask_path, exist_ok=True)\n",
    "            \n",
    "        for img_file in os.listdir(normal_dir):\n",
    "            image_dir = os.path.join(normal_dir, img_file)\n",
    "            foreground_mask_dir = None\n",
    "            if split == 'train' and self.has_fg_mask:\n",
    "                foreground_mask_dir = os.path.join(self.foreground_mask_path, img_file)\n",
    "            self.data.append((image_dir, None, foreground_mask_dir))\n",
    "            \n",
    "        if split == 'test':\n",
    "            test_dir = os.path.join(path, \"test\")\n",
    "            test_anomaly_dirs = []\n",
    "            for entry in os.listdir(test_dir):\n",
    "                full_path = os.path.join(test_dir, entry)\n",
    "\n",
    "                if os.path.isdir(full_path) and full_path != normal_dir:\n",
    "                    test_anomaly_dirs.append(full_path)\n",
    "\n",
    "            for dir in test_anomaly_dirs:\n",
    "                for img_file in os.listdir(dir):\n",
    "                    image_dir = os.path.join(dir, img_file)\n",
    "                    mask_dir = image_dir.replace(\"test\", \"ground_truth\")\n",
    "                    if class_name in [\"01\", \"02\"]:\n",
    "                        mask_dir = mask_dir.replace('.bmp', '.png')\n",
    "                    self.data.append((image_dir, mask_dir, None))\n",
    "\n",
    "            random.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path, fore_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  \n",
    "\n",
    "        image = self.transform(image)          \n",
    "\n",
    "        if mask_path:\n",
    "            mask = Image.open(mask_path).convert('RGB')\n",
    "            mask = self.mask_transform(mask)\n",
    "            mask = 1.0 - torch.all(mask == 0, dim=0).float()\n",
    "            label = 1\n",
    "        else:\n",
    "            C, W, H = image.shape\n",
    "            mask = torch.zeros((H, W))\n",
    "            label = 0\n",
    "            \n",
    "        C, W, H = image.shape\n",
    "        foreground_mask = torch.ones((H, W))\n",
    "        if fore_path:\n",
    "            foreground_mask = Image.open(fore_path).convert('L')\n",
    "            foreground_mask = foreground_mask.resize((self.size, self.size), Image.NEAREST)\n",
    "            foreground_mask = transforms.ToTensor()(foreground_mask)\n",
    "            \n",
    "        return image, label, mask, foreground_mask\n",
    "\n",
    "class VisA(data.Dataset):\n",
    "    def __init__(self, path, class_name, transform=None, mask_transform=None, seed=0, split='train', size=224):\n",
    "        self.path_normal = os.path.join(path, class_name, \"Data\", \"Images\", \"Normal\")\n",
    "        self.path_anomaly = os.path.join(path, class_name, \"Data\", \"Images\", \"Anomaly\")\n",
    "        self.foreground_mask_path = os.path.join(path, class_name, \"Data\", \"Images\", \"foreground_mask\")\n",
    "        self.normal_test = []\n",
    "\n",
    "        self.class_name = class_name\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.data = []\n",
    "        self.size = size\n",
    "        img_count = 0\n",
    "\n",
    "        for filename in os.listdir(self.path_normal):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg')):\n",
    "                img_count += 1\n",
    "                                \n",
    "        for img_path in os.listdir(self.path_normal):\n",
    "            image_dir = os.path.join(self.path_normal, img_path)\n",
    "            foreground_mask_dir = os.path.join(self.foreground_mask_path, img_path)\n",
    "            if not os.path.exists(foreground_mask_dir):\n",
    "                self.normal_test.append(image_dir)\n",
    "                img_count -= 1\n",
    "                                \n",
    "        if split == 'train':\n",
    "            for img_path in os.listdir(self.path_normal):\n",
    "                image_dir = (os.path.join(self.path_normal, img_path))\n",
    "                if image_dir not in self.normal_test:\n",
    "                    foreground_mask_dir = os.path.join(self.foreground_mask_path, img_path)\n",
    "                    self.data.append((image_dir, None, foreground_mask_dir))\n",
    "        elif split == 'test':\n",
    "            for img_path in self.normal_test:\n",
    "                self.data.append((os.path.join(self.path_normal, img_path), None, None)) \n",
    "\n",
    "            for img_path in os.listdir(self.path_anomaly):\n",
    "                image_dir = os.path.join(self.path_anomaly, img_path)\n",
    "                mask_dir = image_dir.replace(\"Images\", \"Masks\")[:-3] + \"png\"\n",
    "                self.data.append((image_dir, mask_dir, None)) \n",
    "\n",
    "            random.shuffle(self.data)            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path, fore_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  \n",
    "\n",
    "        image = self.transform(image)          \n",
    "\n",
    "        if mask_path:\n",
    "            mask = Image.open(mask_path).convert('RGB')\n",
    "            mask = self.mask_transform(mask)\n",
    "            mask = 1.0 - torch.all(mask == 0, dim=0).float()\n",
    "            label = 1\n",
    "        else:\n",
    "            C, W, H = image.shape\n",
    "            mask = torch.zeros((H, W))\n",
    "            label = 0\n",
    "            \n",
    "        C, W, H = image.shape\n",
    "        foreground_mask = torch.ones((H, W))\n",
    "        if fore_path:\n",
    "            foreground_mask = Image.open(fore_path).convert('L')\n",
    "            foreground_mask = foreground_mask.resize((self.size, self.size), Image.NEAREST)\n",
    "            foreground_mask = transforms.ToTensor()(foreground_mask)\n",
    "            \n",
    "        return image, label, mask, foreground_mask\n",
    "    \n",
    "class DTD(data.Dataset):\n",
    "    def __init__(self, path, class_name, transform=None, mask_transform=None, seed=0, split='train', size=224):\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.data = []\n",
    "        self.size = size\n",
    "\n",
    "        path = os.path.join(path, class_name)\n",
    "        mv_str = '_mask.'\n",
    "\n",
    "        normal_dir = os.path.join(path, split, \"good\")\n",
    "            \n",
    "        for img_file in os.listdir(normal_dir):\n",
    "            image_dir = os.path.join(normal_dir, img_file)\n",
    "            self.data.append((image_dir, None, None))\n",
    "            \n",
    "        if split == 'test':\n",
    "            test_dir = os.path.join(path, \"test\")\n",
    "            test_anomaly_dirs = []\n",
    "            for entry in os.listdir(test_dir):\n",
    "                full_path = os.path.join(test_dir, entry)\n",
    "\n",
    "                if os.path.isdir(full_path) and full_path != normal_dir:\n",
    "                    test_anomaly_dirs.append(full_path)\n",
    "\n",
    "            for dir in test_anomaly_dirs:\n",
    "                for img_file in os.listdir(dir):\n",
    "                    image_dir = os.path.join(dir, img_file)\n",
    "                    mask_dir = image_dir.replace(\"test\", \"ground_truth\")\n",
    "                    parts = mask_dir.rsplit('.', 1)\n",
    "                    mask_dir = parts[0] + mv_str + parts[1]\n",
    "                    self.data.append((image_dir, mask_dir, None))\n",
    "\n",
    "            random.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path, fore_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  \n",
    "\n",
    "        image = self.transform(image)          \n",
    "\n",
    "        if mask_path:\n",
    "            mask = Image.open(mask_path).convert('RGB')\n",
    "            mask = self.mask_transform(mask)\n",
    "            mask = 1.0 - torch.all(mask == 0, dim=0).float()\n",
    "            label = 1\n",
    "        else:\n",
    "            C, W, H = image.shape\n",
    "            mask = torch.zeros((H, W))\n",
    "            label = 0\n",
    "            \n",
    "        C, W, H = image.shape\n",
    "        foreground_mask = torch.ones((H, W))\n",
    "        if fore_path:\n",
    "            foreground_mask = Image.open(fore_path).convert('L')\n",
    "            foreground_mask = foreground_mask.resize((self.size, self.size), Image.NEAREST)\n",
    "            foreground_mask = transforms.ToTensor()(foreground_mask)\n",
    "            \n",
    "        return image, label, mask, foreground_mask\n",
    "    \n",
    "class BraTS2021(data.Dataset):\n",
    "    def __init__(self, path, transform=None, mask_transform=None, seed=0, split='train', size=224):\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.data = []\n",
    "        self.size = size\n",
    "\n",
    "        normal_dir = os.path.join(path, split, \"normal\")\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.foreground_mask_path = os.path.join(path, split, \"foreground_mask\")\n",
    "            os.makedirs(self.foreground_mask_path, exist_ok=True)\n",
    "            \n",
    "        for img_file in os.listdir(normal_dir):\n",
    "            image_dir = os.path.join(normal_dir, img_file)\n",
    "            foreground_mask_dir = None\n",
    "            if split == 'train':\n",
    "                foreground_mask_dir = os.path.join(self.foreground_mask_path, img_file)\n",
    "            self.data.append((image_dir, None, foreground_mask_dir))\n",
    "            \n",
    "        if split == 'test':\n",
    "            test_dir = os.path.join(path, \"test\")\n",
    "            test_anomaly_dirs = []\n",
    "            for entry in os.listdir(test_dir):\n",
    "                full_path = os.path.join(test_dir, entry)\n",
    "\n",
    "                if os.path.isdir(full_path) and full_path != normal_dir:\n",
    "                    test_anomaly_dirs.append(full_path)\n",
    "\n",
    "            for dir in test_anomaly_dirs:\n",
    "                for img_file in os.listdir(dir):\n",
    "                    image_dir = os.path.join(dir, img_file)\n",
    "                    mask_dir = os.path.join(path, \"ground_truth\", img_file)\n",
    "                    mask_dir = mask_dir.replace(\"flair\", \"seg\")\n",
    "                    self.data.append((image_dir, mask_dir, None))\n",
    "\n",
    "            random.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path, fore_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  \n",
    "\n",
    "        image = self.transform(image)          \n",
    "\n",
    "        if mask_path:\n",
    "            mask = Image.open(mask_path).convert('RGB')\n",
    "            mask = self.mask_transform(mask)\n",
    "            mask = 1.0 - torch.all(mask == 0, dim=0).float()\n",
    "            label = 1\n",
    "        else:\n",
    "            C, W, H = image.shape\n",
    "            mask = torch.zeros((H, W))\n",
    "            label = 0\n",
    "            \n",
    "        C, W, H = image.shape\n",
    "        foreground_mask = torch.ones((H, W))\n",
    "        if fore_path:\n",
    "            foreground_mask = Image.open(fore_path).convert('L')\n",
    "            foreground_mask = foreground_mask.resize((self.size, self.size), Image.NEAREST)\n",
    "            foreground_mask = transforms.ToTensor()(foreground_mask)\n",
    "            \n",
    "        return image, label, mask, foreground_mask\n",
    "    \n",
    "class HeadCT(data.Dataset):\n",
    "    def __init__(self, path, transform=None, mask_transform=None, seed=0, split='train', size=224):\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.data = []\n",
    "        self.size = size\n",
    "\n",
    "        mv_str = '_mask.'\n",
    "\n",
    "        normal_dir = os.path.join(path, split, \"good\")\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.foreground_mask_path = os.path.join(path, split, \"foreground_mask\")\n",
    "            os.makedirs(self.foreground_mask_path, exist_ok=True)\n",
    "            \n",
    "        for img_file in os.listdir(normal_dir):\n",
    "            image_dir = os.path.join(normal_dir, img_file)\n",
    "            foreground_mask_dir = None\n",
    "            if split == 'train':\n",
    "                foreground_mask_dir = os.path.join(self.foreground_mask_path, img_file)\n",
    "            self.data.append((image_dir, None, foreground_mask_dir))\n",
    "            \n",
    "        if split == 'test':\n",
    "            test_dir = os.path.join(path, \"test\")\n",
    "            test_anomaly_dirs = []\n",
    "            for entry in os.listdir(test_dir):\n",
    "                full_path = os.path.join(test_dir, entry)\n",
    "\n",
    "                if os.path.isdir(full_path) and full_path != normal_dir:\n",
    "                    test_anomaly_dirs.append(full_path)\n",
    "\n",
    "            for dir in test_anomaly_dirs:\n",
    "                for img_file in os.listdir(dir):\n",
    "                    image_dir = os.path.join(dir, img_file)\n",
    "                    mask_dir = image_dir.replace(\"test\", \"ground_truth\")\n",
    "                    parts = mask_dir.rsplit('.', 1)\n",
    "                    mask_dir = parts[0] + mv_str + parts[1]\n",
    "                    self.data.append((image_dir, mask_dir, None))\n",
    "\n",
    "            random.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path, fore_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  \n",
    "\n",
    "        image = self.transform(image)          \n",
    "\n",
    "        if mask_path:\n",
    "            mask = Image.open(mask_path).convert('RGB')\n",
    "            mask = self.mask_transform(mask)\n",
    "            mask = 1.0 - torch.all(mask == 0, dim=0).float()\n",
    "            label = 1\n",
    "        else:\n",
    "            C, W, H = image.shape\n",
    "            mask = torch.zeros((H, W))\n",
    "            label = 0\n",
    "            \n",
    "        C, W, H = image.shape\n",
    "        foreground_mask = torch.ones((H, W))\n",
    "        if fore_path:\n",
    "            foreground_mask = Image.open(fore_path).convert('L')\n",
    "            foreground_mask = foreground_mask.resize((self.size, self.size), Image.NEAREST)\n",
    "            foreground_mask = transforms.ToTensor()(foreground_mask)\n",
    "            \n",
    "        return image, label, mask, foreground_mask\n",
    "    \n",
    "class WFDD(data.Dataset):\n",
    "    def __init__(self, path, class_name, transform=None, mask_transform=None, seed=0, split='train', size=224):\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.data = []\n",
    "        self.size = size\n",
    "\n",
    "        path = os.path.join(path, class_name)\n",
    "        mv_str = '_mask.'\n",
    "\n",
    "        normal_dir = os.path.join(path, split, \"good\")\n",
    "            \n",
    "        for img_file in os.listdir(normal_dir):\n",
    "            image_dir = os.path.join(normal_dir, img_file)\n",
    "            self.data.append((image_dir, None, None))\n",
    "            \n",
    "        if split == 'test':\n",
    "            test_dir = os.path.join(path, \"test\")\n",
    "            test_anomaly_dirs = []\n",
    "            for entry in os.listdir(test_dir):\n",
    "                full_path = os.path.join(test_dir, entry)\n",
    "\n",
    "                if os.path.isdir(full_path) and full_path != normal_dir:\n",
    "                    test_anomaly_dirs.append(full_path)\n",
    "\n",
    "            for dir in test_anomaly_dirs:\n",
    "                for img_file in os.listdir(dir):\n",
    "                    image_dir = os.path.join(dir, img_file)\n",
    "                    mask_dir = image_dir.replace(\"test\", \"ground_truth\")\n",
    "                    parts = mask_dir.rsplit('.', 1)\n",
    "                    mask_dir = parts[0] + mv_str + parts[1]\n",
    "                    self.data.append((image_dir, mask_dir, None))\n",
    "\n",
    "            random.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path, fore_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  \n",
    "\n",
    "        image = self.transform(image)          \n",
    "\n",
    "        if mask_path:\n",
    "            mask = Image.open(mask_path).convert('RGB')\n",
    "            mask = self.mask_transform(mask)\n",
    "            mask = 1.0 - torch.all(mask == 0, dim=0).float()\n",
    "            label = 1\n",
    "        else:\n",
    "            C, W, H = image.shape\n",
    "            mask = torch.zeros((H, W))\n",
    "            label = 0\n",
    "            \n",
    "        C, W, H = image.shape\n",
    "        foreground_mask = torch.ones((H, W))\n",
    "        if fore_path:\n",
    "            foreground_mask = Image.open(fore_path).convert('L')\n",
    "            foreground_mask = foreground_mask.resize((self.size, self.size), Image.NEAREST)\n",
    "            foreground_mask = transforms.ToTensor()(foreground_mask)\n",
    "            \n",
    "        return image, label, mask, foreground_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def patchify(x, patch_size):\n",
    "    if len(x.shape) == 3:  # If single-channel image, add batch dimension\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "    bs, c, h, w = x.shape\n",
    "\n",
    "    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "    x = unfold(x)  # Shape: (B, C * patch_size * patch_size, num_patches)\n",
    "\n",
    "    num_patches = (h // patch_size) * (w // patch_size)\n",
    "    x = x.view(bs, c, patch_size, patch_size, num_patches).permute(0, 4, 1, 2, 3)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def label_patch(x):\n",
    "    labels = torch.any(x > 0, dim=(2, 3, 4)).float()\n",
    "    return labels\n",
    "\n",
    "def get_dataloader(image_size, path, dataset_name, class_name, batch_size, test_batch_size, num_workers, seed):\n",
    "    transform = transforms.Compose([\n",
    "                                    transforms.Resize((image_size, image_size), Image.LANCZOS), \n",
    "                                    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.01), \n",
    "                                    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))], p=0.2),\n",
    "                                    transforms.ToTensor()\n",
    "                                    ])\n",
    "    mask_transform = transforms.Compose([transforms.Resize((image_size, image_size), Image.LANCZOS),transforms.ToTensor()])\n",
    "    \n",
    "    if dataset_name == 'mvtec':\n",
    "        train_set = MVTec(path, class_name, transform=transform, mask_transform=mask_transform, seed=seed, split='train', size=image_size)\n",
    "        test_set = MVTec(path, class_name, transform=transform, mask_transform=mask_transform, seed=seed, split='test', size=image_size)\n",
    "    elif dataset_name == 'visa':\n",
    "        train_set = VisA(path, class_name, transform=transform, mask_transform=mask_transform, seed=seed, split='train', size=image_size)\n",
    "        test_set = VisA(path, class_name, transform=transform, mask_transform=mask_transform, seed=seed, split='test', size=image_size)\n",
    "    elif dataset_name == 'mpdd':\n",
    "        train_set = MPDD(path, class_name, transform=transform, mask_transform=mask_transform, seed=seed, split='train', size=image_size)\n",
    "        test_set = MPDD(path, class_name, transform=transform, mask_transform=mask_transform, seed=seed, split='test', size=image_size)\n",
    "    elif dataset_name == 'btad':\n",
    "        train_set = BTAD(path, class_name, transform=transform, mask_transform=mask_transform, seed=seed, split='train', size=image_size)\n",
    "        test_set = BTAD(path, class_name, transform=transform, mask_transform=mask_transform, seed=seed, split='test', size=image_size)\n",
    "    elif dataset_name == \"dtd\":\n",
    "        train_set = DTD(path, class_name, transform=transform, mask_transform=mask_transform, seed=seed, split='train', size=image_size)\n",
    "        test_set = DTD(path, class_name, transform=transform, mask_transform=mask_transform, seed=seed, split='test', size=image_size)\n",
    "    elif dataset_name == \"brats2021\":\n",
    "        train_set = BraTS2021(path, transform=transform, mask_transform=mask_transform, seed=seed, split='train', size=image_size)\n",
    "        test_set = BraTS2021(path, transform=transform, mask_transform=mask_transform, seed=seed, split='test', size=image_size)\n",
    "    elif dataset_name == \"headct\":\n",
    "        train_set = HeadCT(path, transform=transform, mask_transform=mask_transform, seed=seed, split='train', size=image_size)\n",
    "        test_set = HeadCT(path, transform=transform, mask_transform=mask_transform, seed=seed, split='test', size=image_size)\n",
    "    elif dataset_name == 'wfdd':\n",
    "        train_set = WFDD(path, class_name, transform=transform, mask_transform=mask_transform, seed=seed, split='train', size=image_size)\n",
    "        test_set = WFDD(path, class_name, transform=transform, mask_transform=mask_transform, seed=seed, split='test', size=image_size)\n",
    "\n",
    "    train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers)\n",
    "    test_loader = data.DataLoader(test_set, batch_size=test_batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    print(f\"Dataloaders for dataset {dataset_name} and class {class_name} have been prepared.\")\n",
    "\n",
    "    return train_loader, test_loader\n",
    "    \n",
    "\n",
    "def soft_topk_pooling(scores, k, temperature=1.0):\n",
    "    topk_values, topk_indices = torch.topk(scores, k, dim=1)\n",
    "    weights = torch.softmax(topk_values / temperature, dim=1)\n",
    "    return (topk_values * weights).sum(dim=1)\n",
    "    \n",
    "\n",
    "def get_auc(test_scores, test_labels, test_masks, patches_per_side, sigma, radius, k):\n",
    "    scores = torch.cat(test_scores, dim=0)\n",
    "\n",
    "    topk_values, _ = torch.topk(scores, k, dim=1)\n",
    "    pred_labels = torch.mean(topk_values, dim=1)\n",
    "    image_labels = torch.cat(test_labels, dim=0)\n",
    "\n",
    "    image_auroc = roc_auc_score(image_labels.view(-1).cpu().numpy(), pred_labels.view(-1).cpu().numpy())\n",
    "\n",
    "    masks = torch.cat(test_masks, dim=0)\n",
    "    patch_scores = scores.reshape(-1, patches_per_side, patches_per_side)\n",
    "    pixel_scores = F.interpolate(patch_scores.unsqueeze(1), size=(masks.shape[-1], masks.shape[-1]), mode='bilinear', align_corners=False)\n",
    "    localization = gaussian_filter(pixel_scores.squeeze(1).cpu().detach().numpy(), sigma=sigma, radius=radius, axes=(1,2))\n",
    "\n",
    "    pixel_auroc = roc_auc_score(masks.view(-1).cpu().numpy(), localization.reshape(-1))\n",
    "\n",
    "    return image_auroc, pixel_auroc\n",
    "\n",
    "\n",
    "def save_model(model, filepath=\"./model.pth\"):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model, filepath=\"./model.pth\"):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(filepath))\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def log_loss(epoch, loss, filepath=\"./loss_log.txt\"):\n",
    "    with open(filepath, \"a\") as f:\n",
    "        f.write(f\"Epoch {epoch} : {loss}\\n\")\n",
    "\n",
    "def display_results(metrics_dict, description):\n",
    "    console = Console()\n",
    "    table = Table(title=f\"{description}\")\n",
    "\n",
    "    table.add_column(\"Metric\", style=\"cyan\", justify=\"center\")\n",
    "    table.add_column(\"Value\", style=\"magenta\", justify=\"center\")\n",
    "\n",
    "    for metric, value in metrics_dict.items():\n",
    "        table.add_row(f\"{metric}\", f\"{value:.4f}\")\n",
    "\n",
    "    console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def image_transform(image):\n",
    "     return np.clip(image* 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "def cvt2heatmap(gray):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(gray), cv2.COLORMAP_JET)\n",
    "    return heatmap\n",
    "\n",
    "def show_cam_on_image(img, anomaly_map):\n",
    "    cam = np.float32(anomaly_map)/255 + np.float32(img)/255\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam) \n",
    "\n",
    "def min_max_norm(image):\n",
    "    a_min, a_max = image.min(), image.max()\n",
    "    return (image-a_min)/(a_max - a_min)\n",
    "    \n",
    "def get_heatmap(raw_image, segmentation):\n",
    "    ano_map = gaussian_filter(segmentation, sigma=4)\n",
    "    ano_map = min_max_norm(ano_map)\n",
    "    ano_map = cvt2heatmap(ano_map * 255.0)\n",
    "    raw_image = image_transform(raw_image.detach().cpu().numpy())\n",
    "    image_cv2 = np.uint8(np.transpose(raw_image,(1,2,0)))\n",
    "    ano_map = show_cam_on_image(image_cv2[..., ::-1], ano_map)\n",
    "    ano_map = ano_map[..., ::-1]\n",
    "    return ano_map\n",
    "\n",
    "def transparent_cmap(cmap, N=255):\n",
    "    mycmap = cmap\n",
    "    mycmap._init()\n",
    "    mycmap._lut[:,-1] = np.linspace(0, 0.8, N+4)\n",
    "    return mycmap\n",
    "\n",
    "def visualize_heatmap(args):\n",
    "    device = torch.device(\"cuda\" if args[\"device\"] != \"cpu\" and torch.cuda.is_available() else \"cpu\")\n",
    "    _, test_loader = get_dataloader(args[\"image_size\"], args[\"dataset_dir\"], args[\"dataset\"], args[\"class_name\"], args[\"train_batch_size\"], args[\"test_batch_size\"], args[\"num_workers\"], args[\"seed\"])\n",
    "\n",
    "    model = PatchGuard(args, device)\n",
    "    load_model(model, args[\"checkpoint_dir\"]+f\"patchguard_{args['dataset']}_{args['class_name']}.pth\")    \n",
    "\n",
    "    Path(f\"./plots\").mkdir(exist_ok=True, parents=True)\n",
    "    plot_path = Path(f\"./plots\")\n",
    "\n",
    "    Path(f\"{plot_path}/clean_image\").mkdir(exist_ok=True, parents=True)\n",
    "    clean_image_path = Path(f\"{plot_path}/clean_image\")\n",
    "\n",
    "    Path(f\"{plot_path}/adv_image\").mkdir(exist_ok=True, parents=True)\n",
    "    adv_image_path = Path(f\"{plot_path}/adv_image\")\n",
    "    \n",
    "    Path(f\"{plot_path}/clean_heatmap\").mkdir(exist_ok=True, parents=True)\n",
    "    clean_heatmap_path = Path(f\"{plot_path}/clean_heatmap\")\n",
    "    \n",
    "    Path(f\"{plot_path}/adv_heatmap\").mkdir(exist_ok=True, parents=True)\n",
    "    adv_heatmap_path = Path(f\"{plot_path}/adv_heatmap\")\n",
    "\n",
    "    Path(f\"{plot_path}/mask\").mkdir(exist_ok=True, parents=True)\n",
    "    mask_path = Path(f\"{plot_path}/mask\")\n",
    "    \n",
    "    cmap = transparent_cmap(plt.cm.jet)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for images, _, masks, _ in test_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            for mode in [\"clean\", \"adv\"]:\n",
    "                image_path = clean_image_path if mode == \"clean\" else adv_image_path\n",
    "                heatmap_path = clean_heatmap_path if mode == \"clean\" else adv_heatmap_path\n",
    "\n",
    "                if mode == \"adv\":\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        images = pgd_attack(model, images, label_patch(patchify(masks, model.patch_size)), args[\"epsilon_visualization\"], args[\"step_visualization\"])\n",
    "                        \n",
    "                scores = model(images)\n",
    "    \n",
    "                batch_size, num_patches = scores.shape\n",
    "                image_size = images.shape[-1]\n",
    "                patches_per_side = int(np.sqrt(num_patches))\n",
    "\n",
    "                j = i\n",
    "                for b in range(batch_size):\n",
    "                    patch_scores = scores[b].reshape((patches_per_side, patches_per_side))\n",
    "                    scores_interpolated = F.interpolate(patch_scores.unsqueeze(0).unsqueeze(0),\n",
    "                                                        size=image_size,\n",
    "                                                        mode='bilinear',\n",
    "                                                        align_corners=False\n",
    "                                                        ).squeeze(0).squeeze(0)\n",
    "    \n",
    "                    segmentation = gaussian_filter(scores_interpolated.cpu().detach().numpy(), sigma=args[\"smoothing_sigma\"], radius=args[\"smoothing_radius\"])  \n",
    "                    segmentation = gaussian_filter(segmentation, sigma=4)\n",
    "                    segmentation = min_max_norm(segmentation)\n",
    "\n",
    "                    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "                    ax.imshow(images[b].cpu().detach().permute(1, 2, 0).numpy())\n",
    "                    ax.axis('off')\n",
    "                    plt.savefig(os.path.join(image_path, f'img{j}.png'),  bbox_inches='tight', pad_inches=0, format='png')\n",
    "                    plt.close(fig)\n",
    "                    \n",
    "                    # Save the mask\n",
    "                    if mode == \"clean\":\n",
    "                        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "                        modified_image = images[b].clone()\n",
    "                        modified_image[0, masks[b] > 0] = 1.0\n",
    "                        modified_image[1, masks[b] > 0] = 0.0\n",
    "                        modified_image[2, masks[b] > 0] = 0.0\n",
    "    \n",
    "                        ax.imshow(modified_image.cpu().detach().permute(1, 2, 0).numpy())\n",
    "                        ax.axis('off')\n",
    "                        plt.savefig(os.path.join(mask_path, f'img{j}.png'), bbox_inches='tight', pad_inches=0, format='png')\n",
    "                        plt.close(fig)\n",
    "                    \n",
    "                    # Save the heatmap\n",
    "                    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "                    ax.imshow(images[b].cpu().detach().permute(1, 2, 0).numpy())\n",
    "                    ax.imshow(segmentation, cmap=cmap, interpolation='bilinear')\n",
    "                    ax.axis('off')\n",
    "                    plt.savefig(os.path.join(heatmap_path, f'img{j}.png'), bbox_inches='tight', pad_inches=0, format='png')\n",
    "                    plt.close(fig)\n",
    "    \n",
    "                    j = j + 1\n",
    "\n",
    "            i += args.test_batch_size\n",
    "                \n",
    "        print(\"Visualization complete.\")\n",
    "        shutil.make_archive(f'visualization', 'zip', plot_path)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, device, args, adv_test, epsilon=8/255, steps=10):\n",
    "    model.eval()\n",
    "\n",
    "    test_scores = []\n",
    "    test_labels = []\n",
    "    test_masks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images, labels = batch[0].to(device), batch[1].to(device)\n",
    "            if adv_test:\n",
    "                masks = label_patch(patchify(batch[2], model.patch_size)).to(device)\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    if args[\"attack_type\"] == \"PGD\":\n",
    "                        images = pgd_attack(model, images, masks, epsilon, steps)\n",
    "\n",
    "            masks = batch[2].to(device)\n",
    "            scores = model(images)\n",
    "\n",
    "            test_scores.append(scores.cpu())\n",
    "            test_labels.append(labels.cpu())\n",
    "            test_masks.append(masks.cpu())\n",
    "\n",
    "    image_auc, pixel_auc = get_auc(test_scores, test_labels, test_masks, model.patches_per_side, args[\"smoothing_sigma\"], args[\"smoothing_radius\"], args[\"top_k\"])\n",
    "\n",
    "    return image_auc, pixel_auc\n",
    "\n",
    "def run_test(args):\n",
    "    device = torch.device(\"cuda\" if args[\"device\"] != \"cpu\" and torch.cuda.is_available() else \"cpu\")\n",
    "    model = PatchGuard(args, device).to(device)\n",
    "    load_model(model, args[\"checkpoint_dir\"]+f\"patchguard_{args['dataset']}_{args['class_name']}.pth\")\n",
    "    _, test_loader = get_dataloader(args[\"image_size\"], args[\"dataset_dir\"], args[\"dataset\"], args[\"class_name\"], args[\"train_batch_size\"], args[\"test_batch_size\"], args[\"num_workers\"], args[\"seed\"])\n",
    "\n",
    "    image_auc, pixel_auc = test(model, test_loader, device, args, False)\n",
    "    display_results({\"Image-level AUC\":image_auc, \"Pixel-level AUC\":pixel_auc}, \"Clean Performance\")\n",
    "\n",
    "    if args[\"adv_test\"]:\n",
    "        epsilons = args[\"epsilon_test\"]\n",
    "        step = args[\"step_test\"]\n",
    "\n",
    "        for epsilon in epsilons:\n",
    "            image_auc, pixel_auc = test(model, test_loader, device, args, True, epsilon, step)\n",
    "            display_results({\"Image-level AUC\":image_auc, \"Pixel-level AUC\":pixel_auc}, f\"{args['attack_type']} attack (eps={epsilon}, step={step})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def train_step(model, anomaly_generator, train_loader, optimizer, criterion, use_reg, device, args):\n",
    "    total_sample = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    batch_iterator = tqdm(train_loader, disable=not args[\"use_tqdm\"], desc=\"Training Batches\")\n",
    "    for batch in batch_iterator:\n",
    "        loss = 0\n",
    "\n",
    "        normal_data = [batch[0].to(device)]\n",
    "\n",
    "        if args[\"adv_train\"]:\n",
    "            images = batch[0].to(device).clone()\n",
    "            adv_normal_images = pgd_attack(model, images, torch.zeros(images.shape[0], model.num_patches).to(device), args[\"epsilon_train\"], args[\"step_train\"])\n",
    "            normal_data.append(adv_normal_images)\n",
    "\n",
    "        for imgs in normal_data:\n",
    "            features, attn_weights = model.feature_extractor(imgs, use_reg)\n",
    "\n",
    "            scores_true = model.discriminator(features)\n",
    "\n",
    "            masks_true = torch.zeros(features.shape[0], features.shape[1]).to(device)\n",
    "            loss += criterion(scores_true, masks_true, attn_weights)\n",
    "\n",
    "        images = batch[0].clone()\n",
    "        foreground_masks = batch[3]\n",
    "\n",
    "        augmented_images, augmented_masks = anomaly_generator(images, foreground_masks)\n",
    "        augmented_masks = label_patch(patchify(augmented_masks, model.patch_size))\n",
    "        augmented_images, augmented_masks = augmented_images.to(device), augmented_masks.to(device)\n",
    "\n",
    "        anomaly_data = [augmented_images]\n",
    "        if args[\"adv_train\"]:\n",
    "            adv_distorted_images = pgd_attack(model, augmented_images.clone(), augmented_masks, args[\"epsilon_train\"], args[\"step_train\"])\n",
    "            anomaly_data.append(adv_distorted_images)\n",
    "\n",
    "        for imgs in anomaly_data:\n",
    "            features, attn_weights = model.feature_extractor(imgs, use_reg)\n",
    "            scores_aug = model.discriminator(features)\n",
    "\n",
    "            loss += criterion(scores_aug, augmented_masks, attn_weights)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_sample += (len(anomaly_data) + len(normal_data)) * images.shape[0]\n",
    "        total_loss += loss.item() * images.shape[0]\n",
    "    \n",
    "    return total_loss / total_sample\n",
    "\n",
    "\n",
    "def train(model, anomaly_generator, train_loader, optimizer, lr_scheduler, criterion, use_reg, epochs, device, args):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_iterator = tqdm(range(epochs), disable=not args[\"use_tqdm\"], desc=\"Epochs\")\n",
    "    for epoch in epoch_iterator:\n",
    "        total_loss = train_step(model, anomaly_generator, train_loader, optimizer, criterion, use_reg, device, args)\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "\n",
    "        epoch_iterator.set_postfix(loss=total_loss)\n",
    "        print(f\"Current Epoch {epoch}, Current Loss {total_loss}\")\n",
    "        log_loss(epoch, total_loss)\n",
    "\n",
    "def run_train(args):\n",
    "    set_seed(args[\"seed\"])\n",
    "\n",
    "    device = torch.device(\"cuda\" if args[\"device\"] != \"cpu\" and torch.cuda.is_available() else \"cpu\")\n",
    "    model = PatchGuard(args, device).to(device)\n",
    "\n",
    "    train_loader, _ = get_dataloader(args[\"image_size\"], args[\"dataset_dir\"], args[\"dataset\"], args[\"class_name\"], args[\"train_batch_size\"], args[\"test_batch_size\"], args[\"num_workers\"], args[\"seed\"])\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args[\"lr\"])\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args[\"epochs\"], eta_min=args[\"lr\"] * args[\"lr_decay_factor\"])\n",
    "    \n",
    "    criterion = Loss(args[\"reg_type\"], device, model.num_patches)\n",
    "\n",
    "    anomaly_generator = AnomalyGenerator(args[\"dataset\"], args[\"class_name\"], args[\"seed\"])  \n",
    "\n",
    "    train(model, anomaly_generator, train_loader, optimizer, lr_scheduler, criterion, args[\"use_reg\"], args[\"epochs\"], device, args)\n",
    "\n",
    "    save_model(model, args[\"checkpoint_dir\"]+f\"patchguard_{args['dataset']}_{args['class_name']}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if args[\"mode\"] == \"train\":\n",
    "        run_train(args)\n",
    "    elif args[\"mode\"] == \"test\":\n",
    "        run_test(args)\n",
    "    else:\n",
    "        visualize_heatmap(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"mode\": \"train\",\n",
    "    \"seed\": 0,\n",
    "    \"class_name\": \"\",\n",
    "    \"dataset\": \"\",\n",
    "    \"dataset_dir\": \"\",\n",
    "    \"checkpoint_dir\": \"\",\n",
    "    \"device\": \"cuda\",\n",
    "\n",
    "    \"epochs\": ,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"test_batch_size\": 16,\n",
    "    \"lr\": 0.0008,\n",
    "    \"lr_decay_factor\": 0.0125,\n",
    "    \"lr_adaptor\": 0.0001,\n",
    "    \"wd\": 0.00001,\n",
    "    \"image_size\": 224,\n",
    "    \"num_workers\": 1,\n",
    "    \"use_tqdm\": False,\n",
    "\n",
    "    # Feature extractor config\n",
    "    \"hf_path\": \"vit_small_patch14_dinov2.lvd142m\",\n",
    "    \"feature_layers\": [12],\n",
    "    \"reg_layers\": [6, 9, 12],\n",
    "\n",
    "    # Discriminator config\n",
    "    \"hidden_dim\": 2048,\n",
    "    \"dsc_layers\": 1,\n",
    "    \"dsc_heads\": 4,\n",
    "    \"top_k\": 3,\n",
    "    \"smoothing_sigma\": 6,\n",
    "    \"smoothing_radius\": 7,\n",
    "\n",
    "    # Adversarial attack config\n",
    "    \"attack_type\": \"PGD\",\n",
    "    \"adv_train\": True,\n",
    "    \"adv_test\": True,\n",
    "    \"epsilon_train\": 8/255,  # Normalize\n",
    "    \"epsilon_test\": [4/255, 8/255],  # Normalize\n",
    "    \"epsilon_visualization\": 8/255,  # Normalize\n",
    "    \"step_train\": 10,\n",
    "    \"step_test\": 10,\n",
    "    \"step_visualization\": 10,\n",
    "\n",
    "    # Regularizer config\n",
    "    \"use_reg\": True,\n",
    "    \"reg_type\": \"KL_divergence\",\n",
    "}\n",
    "\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# attack and test\n",
    "args[\"mode\"] = \"test\"\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9179.282287,
   "end_time": "2024-07-26T22:24:13.003916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-26T19:51:13.721629",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
